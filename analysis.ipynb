{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "\n",
    "from emoji import UNICODE_EMOJI\n",
    "from IPython.core.display import display, HTML\n",
    "from tweepy import OAuthHandler, Stream, StreamListener, API as TwApi\n",
    "\n",
    "from util.misc import CARTOGRAM, SKIN_TONES, STATE_LOOKUP, STATES\n",
    "from util.tfidf import tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONSUMER_KEY = os.environ.get('TW_CONSUMER_KEY')\n",
    "CONSUMER_SECRET = os.environ.get('TW_CONSUMER_SECRET')\n",
    "ACCESS_TOKEN_KEY = os.environ.get('TW_ACCESS_TOKEN_KEY')\n",
    "ACCESS_TOKEN_SECRET = os.environ.get('TW_ACCESS_TOKEN_SECRET')\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "USA_BBOX = [-175.1, 22.4, -59.8, 72.3]  # via http://boundingbox.klokantech.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a few helper functions\n",
    "\n",
    "def extract_emojis(txt):\n",
    "    return [c for c in txt if c in UNICODE_EMOJI]\n",
    "\n",
    "def sort_values(data):\n",
    "    return sorted(data.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def get_json(fname):\n",
    "    with open('{}/{}'.format(DATA_DIR, fname)) as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def save_to_json(data, fname):\n",
    "    with open('{}/{}'.format(DATA_DIR, fname), 'w') as f:\n",
    "        json.dump(data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step 1: fetch tweets (within USA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this uses Twitter's streaming API\n",
    "# for demo purposes, this only fetches 1k tweets\n",
    "# (in reality, this ran over several days and collected millions of tweets)\n",
    "\n",
    "class MyListener(StreamListener):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ct = 0\n",
    "        self.started = time()\n",
    "\n",
    "    def on_status(self, data):\n",
    "        if hasattr(data, 'retweeted_status'):\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with open('{}/tweets-all.json'.format(DATA_DIR), 'a') as f:\n",
    "                f.write('{}\\n'.format(json.dumps(data._json)))\n",
    "        except Exception as e:\n",
    "            print('error: {}'.format(str(e)))\n",
    "\n",
    "        self.ct += 1\n",
    "        if (self.ct % 100 == 0):\n",
    "            print('üö® {} tweets... ({} secs elapsed)'.format(\n",
    "                self.ct,\n",
    "                int((time() - self.started))\n",
    "            ))\n",
    "\n",
    "        # stop stream after 1k results (for demo)\n",
    "        if self.ct > 1000:\n",
    "            return False\n",
    "            \n",
    "    def on_error(self, status):\n",
    "        print('uh-oh! ({})'.format(status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® 100 tweets... (4 secs elapsed)\n",
      "üö® 200 tweets... (8 secs elapsed)\n",
      "üö® 300 tweets... (13 secs elapsed)\n",
      "üö® 400 tweets... (17 secs elapsed)\n",
      "üö® 500 tweets... (21 secs elapsed)\n",
      "üö® 600 tweets... (26 secs elapsed)\n",
      "üö® 700 tweets... (29 secs elapsed)\n",
      "üö® 800 tweets... (34 secs elapsed)\n",
      "üö® 900 tweets... (38 secs elapsed)\n",
      "üö® 1000 tweets... (42 secs elapsed)\n"
     ]
    }
   ],
   "source": [
    "auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN_KEY, ACCESS_TOKEN_SECRET)\n",
    "api = TwApi(auth)\n",
    "\n",
    "stream = Stream(auth, MyListener())\n",
    "stream.filter(locations=USA_BBOX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step 2: filter tweets to ones containing emojis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with 0...\n",
      "done with 200000...\n",
      "done with 400000...\n",
      "done with 600000...\n",
      "done with 800000...\n",
      "done with 1000000...\n",
      "done with 1200000...\n",
      "done with 1400000...\n",
      "done with 1600000...\n",
      "done with 1800000...\n",
      "413433 tweets with emojis\n"
     ]
    }
   ],
   "source": [
    "filtered = []\n",
    "\n",
    "with open('{}/tweets-all.json'.format(DATA_DIR)) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        d = json.loads(line)\n",
    "        emojis = extract_emojis(d['text'])\n",
    "\n",
    "        if i % 200000 == 0:\n",
    "            print('done with {}...'.format(i))\n",
    "\n",
    "        if not len(emojis):\n",
    "            continue\n",
    "\n",
    "        filtered.append({\n",
    "            'id': d['id_str'],\n",
    "            'time': d['created_at'],\n",
    "            'user': d['user']['screen_name'],\n",
    "            'text': d['text'],\n",
    "            'coordinates': d['coordinates'],\n",
    "            'place': d['place'],\n",
    "            'emojis': emojis,\n",
    "            'emojis_names': [UNICODE_EMOJI[e] for e in emojis],\n",
    "        })\n",
    "\n",
    "print('{} tweets with emojis'.format(len(filtered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_to_json(filtered, 'tweets-w-emojis.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step 3: aggregate by emoji and state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coordinates': None,\n",
       " 'emojis': ['üëÄ', 'üòè'],\n",
       " 'emojis_names': [':eyes:', ':smirking_face:'],\n",
       " 'id': '866461716569350145',\n",
       " 'place': {'attributes': {},\n",
       "  'bounding_box': {'coordinates': [[[-85.605166, 30.355644],\n",
       "     [-85.605166, 35.000771],\n",
       "     [-80.742567, 35.000771],\n",
       "     [-80.742567, 30.355644]]],\n",
       "   'type': 'Polygon'},\n",
       "  'country': 'United States',\n",
       "  'country_code': 'US',\n",
       "  'full_name': 'Georgia, USA',\n",
       "  'id': '7142eb97ae21e839',\n",
       "  'name': 'Georgia',\n",
       "  'place_type': 'admin',\n",
       "  'url': 'https://api.twitter.com/1.1/geo/id/7142eb97ae21e839.json'},\n",
       " 'text': \"I'm here for Drake and Vanessa... Dranessa üëÄüòè\",\n",
       " 'time': 'Mon May 22 01:12:25 +0000 2017',\n",
       " 'user': 'LongLiveDenzy'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_json('tweets-w-emojis.json')\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_cts, emoji_cts = defaultdict(int), defaultdict(int)\n",
    "state_emoji_cts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for d in data:\n",
    "    place = d['place']\n",
    "\n",
    "    if not place:\n",
    "        continue\n",
    "\n",
    "    country, ptype = place['country_code'], place['place_type']\n",
    "    if country != 'US' or ptype not in ['city', 'admin']:\n",
    "        continue\n",
    "\n",
    "    state = place['name']\n",
    "    if ptype == 'city':\n",
    "        state = STATE_LOOKUP[place['full_name'][-2:].upper()]\n",
    "\n",
    "    if state not in STATES:\n",
    "        continue\n",
    "\n",
    "    state_cts[state] += 1\n",
    "    for e in d['emojis_names']:\n",
    "        if e not in SKIN_TONES:\n",
    "            emoji_cts[e] += 1\n",
    "            state_emoji_cts[state][e] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Texas', 55023),\n",
       " ('California', 48004),\n",
       " ('Florida', 26849),\n",
       " ('New York', 20858),\n",
       " ('Ohio', 18172),\n",
       " ('Georgia', 17905),\n",
       " ('Illinois', 13705),\n",
       " ('Louisiana', 13586),\n",
       " ('North Carolina', 11725),\n",
       " ('Pennsylvania', 10869)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_values(state_cts)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(':face_with_tears_of_joy:', 133771),\n",
       " (':loudly_crying_face:', 47571),\n",
       " (':red_heart:', 30301),\n",
       " (':smiling_face_with_heart-eyes:', 28715),\n",
       " (':fire:', 18188),\n",
       " (':female_sign:', 17195),\n",
       " (':weary_face:', 15559),\n",
       " (':skull:', 13885),\n",
       " (':face_with_rolling_eyes:', 13747),\n",
       " (':person_shrugging:', 12666)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_values(emoji_cts)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alabama:\n",
      "[(':face_with_tears_of_joy:', 2882), (':loudly_crying_face:', 590), (':female_sign:', 498)]\n",
      "Alaska:\n",
      "[(':face_with_tears_of_joy:', 80), (':red_heart:', 50), (':loudly_crying_face:', 33)]\n",
      "Arizona:\n",
      "[(':face_with_tears_of_joy:', 1985), (':loudly_crying_face:', 745), (':red_heart:', 718)]\n",
      "Arkansas:\n",
      "[(':face_with_tears_of_joy:', 958), (':fire:', 258), (':red_heart:', 221)]\n",
      "California:\n",
      "[(':face_with_tears_of_joy:', 13933), (':loudly_crying_face:', 5584), (':red_heart:', 4001)]\n"
     ]
    }
   ],
   "source": [
    "# show most popular emojis by state \n",
    "\n",
    "results = []\n",
    "\n",
    "for state, emojis in sorted(state_emoji_cts.items()):\n",
    "    results.append({\n",
    "        'state': state,\n",
    "        'emojis': dict(emojis),\n",
    "        'top_emojis': sort_values(emojis)[:10],\n",
    "    })\n",
    "\n",
    "for r in results[:5]:\n",
    "    print('{}:\\n{}'.format(r['state'], r['top_emojis'][:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_to_json(results, 'emojis-by-state.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step 4: add tf-idf (term frequency‚Äìinverse document frequency)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = get_json('emojis-by-state.json')\n",
    "counts_list = [d['emojis'] for d in data]\n",
    "\n",
    "data_tfidf = []\n",
    "\n",
    "for d in data:    \n",
    "    counts = d['emojis']\n",
    "    scores = {word: tfidf(word, counts, counts_list) for word in counts}\n",
    "    sorted_words = sort_values(scores)\n",
    "\n",
    "    data_tfidf.append({\n",
    "        'state': d['state'],\n",
    "        'top_counts': d['top_emojis'],\n",
    "        'top_tfidf': sorted_words[:10],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alabama:\n",
      "\t1. :double_exclamation_mark: tf-idf: 0.00104\n",
      "\t2. :speaking_head: tf-idf: 0.00094\n",
      "\t3. :cat_face_with_tears_of_joy: tf-idf: 0.00074\n",
      "Alaska:\n",
      "\t1. :snow-capped_mountain: tf-idf: 0.00971\n",
      "\t2. :mount_fuji: tf-idf: 0.0053\n",
      "\t3. :passenger_ship: tf-idf: 0.00486\n",
      "Arizona:\n",
      "\t1. :A_button_(blood_type): tf-idf: 0.00311\n",
      "\t2. :deciduous_tree: tf-idf: 0.00198\n",
      "\t3. :cactus: tf-idf: 0.00138\n",
      "Arkansas:\n",
      "\t1. :cloud: tf-idf: 0.01442\n",
      "\t2. :double_exclamation_mark: tf-idf: 0.00209\n",
      "\t3. :water_wave: tf-idf: 0.00207\n",
      "California:\n",
      "\t1. :heavy_minus_sign: tf-idf: 0.00147\n",
      "\t2. :thermometer: tf-idf: 0.0009\n",
      "\t3. :cat_face_with_tears_of_joy: tf-idf: 0.00068\n"
     ]
    }
   ],
   "source": [
    "# preview results\n",
    "\n",
    "for d in data_tfidf[:5]:\n",
    "    print('{}:'.format(d['state']))\n",
    "\n",
    "    for i, result in enumerate(d['top_tfidf'][:3]):\n",
    "        word, score = result\n",
    "        print(\"\\t{}. {} tf-idf: {}\".format(i + 1, word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step 5: combine with state cartogram info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoji_stats = { d['state']: d for d in data_tfidf }\n",
    "data_final = []\n",
    "\n",
    "for c in CARTOGRAM:\n",
    "    entry = c.copy()\n",
    "    entry.update({'stats': emoji_stats[c['name']]})\n",
    "    data_final.append(entry)\n",
    "\n",
    "save_to_json(data_final, 'state-stats.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
